# This is a basic workflow that is manually triggered
name: Properties collector

on:
  workflow_dispatch:
    inputs:
      startid:
        description: "startid is the first property id to be collected"
        default: 2019300000
        required: true
        type: int
      endid:
        description: 'endid-1 is the last property id to be collected'
        default: 2019400000
        required: true
        type: int
      chunksize:
        description: 'chunksize is the number of ids to be processed by each job'
        default: 5000
        required: true
        type: int

jobs:
  gen_intarray:
    outputs:
      intarray: ${{ steps.intarray.outputs.intarray }}

    runs-on: ubuntu-latest
    steps:
    - uses: actions/checkout@v4

    - name: generate int array
      id: intarray
      run: |
        export NUMBER_CHUNKS=$(((${{ inputs.endid }} - ${{ inputs.startid }}) / ${{ inputs.chunksize }}))
        export INT_ARRAY="[$(seq 0 1 $NUMBER_CHUNKS | sed -z 's/\n/, /g' | sed 's/..$//')]"
        echo intarray=$INT_ARRAY >> "$GITHUB_OUTPUT"
        cat "$GITHUB_OUTPUT"

  scrape:
    needs: gen_intarray
    strategy:
      matrix:
        offset: ${{ fromJSON(needs.gen_intarray.outputs.intarray) }}
    
    runs-on: ubuntu-latest
    permissions:
      contents: 'read'
      id-token: 'write'
    steps:
    - uses: actions/checkout@v4
      with:
        repository: "haroldmei/webscrapers"
        token: ${{ secrets.PRIVATE_SSH_KEY }}

    - uses: actions/setup-python@v5
      with:
        python-version: '3.10' 
    - name: Install Packages and Python dependencies
      run: |
        python -m pip install --upgrade pip pandas pandas_gbq pytube requests joblib scrapy
        apt install parallel -y
    - uses: 'google-github-actions/auth@v2'
      with:
        service_account: 'sunlit-core-205604@sunlit-core-205604.iam.gserviceaccount.com'
        workload_identity_provider: 'projects/570510735077/locations/global/workloadIdentityPools/github/providers/yt-comment-analysis'

    - uses: google-github-actions/setup-gcloud@v2
    - name: Run gcloud
      run: |
        gcloud compute instances list
        gsutil -i sunlit-core-205604@sunlit-core-205604.iam.gserviceaccount.com ls gs://hmei-bucket

    - name: execute py script
      run: |
        export BASE_PROPERTY_ID=${{ inputs.startid }}
        export PROPERTY_ID=$(($BASE_PROPERTY_ID + ${{ matrix.offset }}*${{ inputs.chunksize }}))
        export LAST_PROPERTY_ID=$(($PROPERTY_ID + ${{ inputs.chunksize }}))
        echo ${{ matrix.offset }} ${{ inputs.chunksize }} $BASE_PROPERTY_ID $PROPERTY_ID $LAST_PROPERTY_ID
        cd sales_details
        printf '%s\n' scrapy\ crawl\ sale\ -a\ id={$PROPERTY_ID..$LAST_PROPERTY_ID} | parallel --pipe -N 1 bash
    
    - name: save results
      run: |
        cd sales_details
        gsutil -m cp -r outputs gs://hmei-bucket/data/
        gsutil -m cp -r images gs://hmei-bucket/data/

# conclude
  conclude:
    needs: scrape
    runs-on: ubuntu-latest
    steps:
    - name: Conclude here
      run: lscpu; lsmem; echo "Finished scraping ${{ inputs.youtuber }}:${{ inputs.channel }}"